---
title: "Hierachical Edge Bundles in R"
date: `r Sys.Date()`
time: "20:00"
excerpt: "An introduction to my R implementation of Hierachical Edge Bundles"
author: "Anders Ellern Bilgrau"
draft: false
toc: true
tags:
  - hej
  - med
  - dig
images:
  - /network2.jpg
output: 
  html_document: 
    keep_md: yes
    number_sections: yes
    toc: yes
---

```{r setup, echo = FALSE}
set.seed(754625961)
library("kableExtra")
options(kableExtra.html.bsTable = TRUE)
```

{{< figure src="/images/network2.jpg" alt="image" caption="Hierarchical edge bundling of a gene network" class="big" >}}

## Background

During the finalisation of my PhD, I implemented the so-called *Hierachical Edge Bundling* plotting method for a paper now [published in the Annals of Applied Statistics](https://projecteuclid.org/euclid.aoas/1536652979). 
Though the plot above sadly did not make the final cut, I admit due to an lack of added value, it appeared in the orignal submisson and the thesis. 
Suffice it to say, I was still quite pleased with the method at the time, so I wrapped the methods into an **R**-package called [**HierachicalEdgeBundles**](https://github.com/AEBilgrau/HierarchicalEdgeBundles). 
The plan was then to do a very small paper on this topic but I never got around to it.

The plot above illustrates estimated correlations of selected genes within the diease called diffuse large B-cell lymphoma the method.


## What is hierachical edge bundling?
Hierachical edge bundling is a visualisation technique for displaying the relations in a network that additionally submit to some hierachical structure.
The method was originally suggested in 2006 in a [lovely paper by Danny Holten](https://ieeexplore.ieee.org/document/4015425).
In short, hierarchical edge bundling visualizes graphs (of nodes and edges) by guiding the edges along along a hierarchical tree of the nodes.
A bundling parameter then controls how tightly the edges follow this underlying tree. 
So what data fits into this technique?

### The data 
The typical data suitable for this visualisation needs two basic properties:

1. The data can be represented as a graph (directed or undirected)
2. The data can be arranged hierachically 

If the data does not adhere to one of these properties the lacking properties may often be constructed. 
Now, I say 'data' here. But by 'data' I mean the likely processed data that goes in to the plotting method, not nessesarily the *collected* data. 

Let me illustrate by using the **R** package.


## Using the **HierarchicalEdgeBundles** package

### Installation

First things first. To install the package directly from the [repository on GitHub](https://github.com/AEBilgrau/HierarchicalEdgeBundles), we run:

```{r package_installation, eval=FALSE}
#install.packages("remotes")  # Uncomment if devtools is not installed
remotes::install_github("AEBilgrau/HierarchicalEdgeBundles")
```


### Use some toy data

To illustrate the package use, we use a typical go-to dataset in **R**. 
The `mtcars` dataset:

```{r mtcars_show, eval=FALSE}
data(mtcars)
head(mtcars, n = 10)
```
```{r mtcars_pretty_print, results='asis', echo = FALSE}
head(mtcars, n = 10) %>% 
   kable(format = "markdown", align = "c")  # Pretty printing
```

The `mtcars` contains data for various specs of 32 cars where only the `10` first cars are shown above. 
Let's suppose that we would like to examine the 'closeness' of these cars to each other.
So it is natural to evaluate all pairs of cars by some metric. 
Using the euclidean metric, say, we get:


```{r}
car_dist <- dist(mtcars) # Compute pairwise distances 

# Show first four rows and columns 
round(as.matrix(car_dist)[1:4, 1:4], 1)
```

These distances are measures of 'dissimilarity' between the cars. 
Greater value corresponds to a greater dissimilarity of the cars.
We do not bother here to consider if the euclidian metrics is a well-suited measure for this (it is likely not).
Nonetheless, since `car_dist` is a symmetric matrix it can be represented by a complete weighted graph. Using the `igraph`-package, we da display it as such easily:

```{r car_dist_plot, message=FALSE}
library("igraph")

car_graph <- graph_from_adjacency_matrix(
  as.matrix(car_dist), mode = "undirected", weighted = TRUE, diag = FALSE
)

# map values linearly to [0, 1]
rescale <- function(x) {  
  return((x - min(x))/max(x - min(x)))
}

E(car_graph)$width <- 5*rescale(E(car_graph)$weight)
E(car_graph)$color <- hsv(h = 0.02, s = 0.6, alpha = rescale(E(car_graph)$weight))
V(car_graph)$size <- 10
V(car_graph)$color <- "steelblue"

par(mar = c(0, 0, 0, 0))
plot(car_graph)
```

The first of our two ingredients.

Using the  dissimilarities we can, for example, use regular hierachical clustering to arrive a the second ingredient:

```{r}
car_hc <- hclust(car_dist, "ave")
plot(car_hc)
```

A tree.

We happily notice that similar cars indeed cluster together: Merc 450's are closely connceted; as are some automakers such as Fiat and Mazda. 



```{r, plots}
# library("HierarchicalEdgeBundles")
# library("igraph")
# library("ape")
# 
# graph <- watts.strogatz.game(1, size = 10, nei = 2, p = 0.5)
# adj.mat <- get.adjacency(graph)
# phylo <- as.phylo(hclust(as.dist(1-adj.mat)))
# 
# plot(graph, layout = layout_in_circle(graph))
# plotHEB(graph, phylo, type = "fan")
```



## References

* Danny Holten (2006) [**"Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data"**](https://ieeexplore.ieee.org/document/4015425), IEEE Transactions on Visualization and Computer Graphics, 12 (5): 741-748.

